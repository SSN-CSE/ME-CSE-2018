* MACHINE LEARNING TECHNIQUES 
:properties:
:author: S Rajalakshmi, B Senthil Kumar
:date: 26 June 2018
:end:

{{{credits}}}
|L|T|P|C|
|3|0|0|3|

** Course Objectives
- To have a basic knowledge in the concepts and techniques of machine learning 
- To have understanding on the working of various machine learning algorithms 
- To use the various probability based learning techniques and evolutionary models
- To understand graphical models of machine learning algorithms 

{{{unit}}}
|Unit I |Introduction|8|
Learning: Types of machine learning -- Design a learning system --
Perspectives and issues in machine learning; Concept Learning Task:
Concept learning as search -- Finding a maximally specific hypothesis
-- Version spaces and the Candidate Elimination algorithm; Curse of
dimensionality -- Overfitting -- Bias-variance tradeoff

\begin{comment}
(Linear Discriminants – Perceptron – Linear Separability – Linear Regression) are moved to second unit. (Curse of Dimensionality -- Overfitting -- Bias-variance tradeoff) are added.
\end{comment}

{{{unit}}}
|Unit II|Linear and Non-Linear Models|10| 
The Brain and the Neuron -- Perceptron -- Linear separability --
Linear Regression; Multi-Layer Perceptron: Going forwards -- Going
backwards Back Propagation error -- Multi-layer perceptron in Practice
– Examples of using the MLP – Deriving back-propagation; Radial Basis
Functions and Splines: Concepts -- RBF Network; Support Vector
Machines: Kernels

\begin{comment}
(Curse of Dimensionality) is moved to first unit. (Interpolations and Basis Functions) are removed. (Kernel methods) is added.
\end{comment}

{{{unit}}}
|Unit III|Tree and Probabilistic Models |9| 
Learning with Trees: Decision trees -- Constructing decision trees --
Classification and regression trees; Ensemble Learning: Boosting --
Bagging -- Different ways to Combine Classifiers; Probabilistic
Learning: Gaussian Mixture Models -- Nearest neighbor methods;
Unsupervised Learning: K-means algorithms

\begin{comment}
(Vector quantization) is removed. (Data into Probabilities – Basic Statistics) are removed.
\end{comment}

{{{unit}}}
|Unit IV|Dimensionality Reduction and Evolutionary Models |9| 
Dimensionality Reduction: Linear discriminant analysis -- Principal
component analysis -- Independent component analysis; Evolutionary
Learning: Genetic algorithms -- Genetic offspring -- Genetic operators
-- Using Genetic algorithms; Reinforcement Learning: Getting lost
example -- Markov decision process

\begin{comment}
(Factor Analysis – Locally Linear Embedding – Isomap – Least Squares Optimization) are removed.
\end{comment}

{{{unit}}}
|Unit V|Graphical Models |9|
Markov Chain Monte Carlo Methods: Sampling -- Proposal distribution --
Markov Chain Monte Carlo; Graphical Models: Bayesian networks --
Markov random fields -- Hidden markov models

\begin{comment}
(Markov Random Fields – Tracking Methods) are removed.
\end{comment}

\hfill *Total: 45*

** Course Outcomes
Upon the completion of the course the students should be able to: 
- Understand the basic concepts of machine learning (K2)
- Analyze the real world problems using linear and non-linear techniques (K4)
- Solve the applications using tree and probabilistic models (K3)
- Apply the various dimensionality reduction techniques and evolutionary models (K3)
- Understand the concepts of graphical models (K2)
      
** References
1. Stephen Marsland, ``Machine Learning – An Algorithmic
   Perspective'', Second Edition, Chapman and Hall/CRC Machine
   Learning and Pattern Recognition Series, 2014.
2. Tom M Mitchell, ``Machine Learning, First Edition'', McGraw Hill
   Education, 2013.
3. Ethem Alpaydin, ``Introduction to Machine Learning 3e (Adaptive
   Computation and Machine Learning Series)'', Third Edition, MIT
   Press, 2014
4. Jason Bell, ``Machine learning – Hands on for Developers and
   Technical Professionals'', First Edition, Wiley, 2014
5. Peter Flach, ``Machine Learning: The Art and Science of Algorithms
   that Make Sense of Data'', First Edition, Cambridge University
   Press, 2012.
